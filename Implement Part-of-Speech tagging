import nltk
from nltk.tokenize import word_tokenize from nltk.tag import pos_tag
# Download necessary NLTK data (if not already downloaded) nltk.download('averaged_perceptron_tagger_eng') # Changed to download 'averaged_perceptron_tagger_eng'
# Sample sentence
sentence = "The quick brown fox jumps over the lazy dog." # Step 1: Tokenize the sentence
tokens = word_tokenize(sentence) print("Tokens:", tokens)
# Step 2: Perform Part-of-Speech (POS) tagging pos_tags = pos_tag(tokens)
print("POS Tags:", pos_tags)
